# Code Quality and Maintainability Enhancements

## üîç Current Issues Analysis

Based on the recent scraping error for `khaleejtimes.com` ("socket hang up") and codebase review, here are key areas for improvement:

### üö® Immediate Issues

1. **Network Resilience**: Socket hang up errors indicate connection instability
2. **Error Handling**: Limited retry mechanisms for transient failures
3. **Resource Management**: Browser instances may not be properly cleaned up
4. **Monitoring**: Insufficient logging for debugging network issues

## üõ†Ô∏è Recommended Enhancements

### 1. **Enhanced Error Handling & Retry Logic**

#### Current Issue
```javascript
// Current basic error handling
catch (error) {
    console.error('Error scraping news:', error);
    throw error;
}
```

#### Recommended Solution
```javascript
// Enhanced retry mechanism with exponential backoff
class ScrapingError extends Error {
    constructor(message, type, url, retryable = false) {
        super(message);
        this.type = type;
        this.url = url;
        this.retryable = retryable;
        this.timestamp = new Date().toISOString();
    }
}

async function scrapeWithRetry(url, options = {}, maxRetries = 3) {
    const { baseDelay = 1000, maxDelay = 10000 } = options;
    
    for (let attempt = 1; attempt <= maxRetries; attempt++) {
        try {
            return await scrapeTopNews(url, options);
        } catch (error) {
            const isRetryable = isRetryableError(error);
            
            if (!isRetryable || attempt === maxRetries) {
                throw new ScrapingError(
                    error.message,
                    getErrorType(error),
                    url,
                    isRetryable
                );
            }
            
            const delay = Math.min(baseDelay * Math.pow(2, attempt - 1), maxDelay);
            console.warn(`Attempt ${attempt} failed for ${url}, retrying in ${delay}ms...`);
            await new Promise(resolve => setTimeout(resolve, delay));
        }
    }
}

function isRetryableError(error) {
    const retryableErrors = [
        'socket hang up',
        'ECONNRESET',
        'ETIMEDOUT',
        'ENOTFOUND',
        'net::ERR_',
        'TimeoutError'
    ];
    
    return retryableErrors.some(pattern => 
        error.message.includes(pattern)
    );
}

function getErrorType(error) {
    if (error.message.includes('socket hang up')) return 'NETWORK_DISCONNECTION';
    if (error.message.includes('timeout')) return 'TIMEOUT';
    if (error.message.includes('ECONNRESET')) return 'CONNECTION_RESET';
    if (error.message.includes('net::ERR_')) return 'BROWSER_NETWORK_ERROR';
    return 'UNKNOWN';
}
```

### 2. **Improved Resource Management**

#### Browser Pool Implementation
```javascript
class BrowserPool {
    constructor(maxBrowsers = 3) {
        this.maxBrowsers = maxBrowsers;
        this.browsers = [];
        this.activeBrowsers = 0;
        this.queue = [];
    }
    
    async getBrowser() {
        if (this.browsers.length > 0) {
            return this.browsers.pop();
        }
        
        if (this.activeBrowsers < this.maxBrowsers) {
            this.activeBrowsers++;
            return await this.createBrowser();
        }
        
        // Wait for available browser
        return new Promise((resolve) => {
            this.queue.push(resolve);
        });
    }
    
    async releaseBrowser(browser) {
        if (this.queue.length > 0) {
            const resolve = this.queue.shift();
            resolve(browser);
        } else {
            this.browsers.push(browser);
        }
    }
    
    async createBrowser() {
        return await puppeteer.launch({
            headless: 'new',
            args: [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-gpu',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        });
    }
    
    async cleanup() {
        const allBrowsers = [...this.browsers];
        this.browsers = [];
        
        await Promise.all(
            allBrowsers.map(browser => 
                browser.close().catch(console.error)
            )
        );
    }
}

// Usage
const browserPool = new BrowserPool(2);

async function scrapeWithPool(url, options) {
    const browser = await browserPool.getBrowser();
    try {
        const page = await browser.newPage();
        // ... scraping logic
        await page.close();
        return results;
    } finally {
        await browserPool.releaseBrowser(browser);
    }
}
```

### 3. **Enhanced Monitoring & Logging**

#### Structured Logging Implementation
```javascript
class Logger {
    constructor(level = 'info') {
        this.level = level;
        this.levels = { error: 0, warn: 1, info: 2, debug: 3 };
    }
    
    log(level, message, meta = {}) {
        if (this.levels[level] <= this.levels[this.level]) {
            const logEntry = {
                timestamp: new Date().toISOString(),
                level: level.toUpperCase(),
                message,
                ...meta
            };
            
            console.log(JSON.stringify(logEntry));
            
            // Optional: Write to file for production
            if (process.env.NODE_ENV === 'production') {
                this.writeToFile(logEntry);
            }
        }
    }
    
    error(message, meta) { this.log('error', message, meta); }
    warn(message, meta) { this.log('warn', message, meta); }
    info(message, meta) { this.log('info', message, meta); }
    debug(message, meta) { this.log('debug', message, meta); }
    
    writeToFile(logEntry) {
        const logDir = path.join(__dirname, 'logs');
        if (!fs.existsSync(logDir)) {
            fs.mkdirSync(logDir, { recursive: true });
        }
        
        const logFile = path.join(logDir, `app-${new Date().toISOString().split('T')[0]}.log`);
        fs.appendFileSync(logFile, JSON.stringify(logEntry) + '\n');
    }
}

const logger = new Logger(process.env.LOG_LEVEL || 'info');

// Usage in scraping functions
async function scrapeTopNews(url, options = {}) {
    const startTime = Date.now();
    const requestId = Math.random().toString(36).substr(2, 9);
    
    logger.info('Starting scrape operation', {
        url,
        requestId,
        options: { ...options, sensitive: '[REDACTED]' }
    });
    
    try {
        // ... scraping logic
        
        logger.info('Scrape operation completed', {
            url,
            requestId,
            duration: Date.now() - startTime,
            articlesFound: newsArticles.length
        });
        
        return newsArticles;
    } catch (error) {
        logger.error('Scrape operation failed', {
            url,
            requestId,
            duration: Date.now() - startTime,
            error: error.message,
            stack: error.stack
        });
        throw error;
    }
}
```

### 4. **Configuration Management**

#### Environment-based Configuration
```javascript
// config/index.js
const config = {
    scraping: {
        maxNews: parseInt(process.env.MAX_NEWS) || 10,
        timeout: parseInt(process.env.SCRAPING_TIMEOUT) || 30000,
        delayBetweenRequests: parseInt(process.env.DELAY_BETWEEN_REQUESTS) || 2000,
        maxRetries: parseInt(process.env.MAX_RETRIES) || 3,
        userAgent: process.env.USER_AGENT || 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        viewport: {
            width: parseInt(process.env.VIEWPORT_WIDTH) || 1920,
            height: parseInt(process.env.VIEWPORT_HEIGHT) || 1080
        }
    },
    browser: {
        headless: process.env.BROWSER_HEADLESS !== 'false',
        maxBrowsers: parseInt(process.env.MAX_BROWSERS) || 3,
        args: [
            '--no-sandbox',
            '--disable-setuid-sandbox',
            '--disable-dev-shm-usage',
            '--disable-gpu'
        ]
    },
    logging: {
        level: process.env.LOG_LEVEL || 'info',
        enableFileLogging: process.env.ENABLE_FILE_LOGGING === 'true'
    },
    websites: {
        khaleejtimes: {
            url: 'https://www.khaleejtimes.com/',
            selectors: {
                articles: 'article, .news-item, .story',
                title: 'h1, h2, h3, .headline',
                link: 'a',
                summary: 'p, .summary, .excerpt'
            },
            specialHandling: {
                requiresJavaScript: true,
                hasAntiBot: true,
                cookieConsent: true
            }
        }
    }
};

module.exports = config;
```

### 5. **Health Monitoring & Metrics**

#### Performance Metrics Collection
```javascript
class MetricsCollector {
    constructor() {
        this.metrics = {
            totalRequests: 0,
            successfulRequests: 0,
            failedRequests: 0,
            averageResponseTime: 0,
            errorsByType: {},
            websiteHealth: {}
        };
    }
    
    recordRequest(url, success, duration, error = null) {
        this.metrics.totalRequests++;
        
        if (success) {
            this.metrics.successfulRequests++;
        } else {
            this.metrics.failedRequests++;
            
            if (error) {
                const errorType = getErrorType(error);
                this.metrics.errorsByType[errorType] = 
                    (this.metrics.errorsByType[errorType] || 0) + 1;
            }
        }
        
        // Update average response time
        this.metrics.averageResponseTime = 
            (this.metrics.averageResponseTime * (this.metrics.totalRequests - 1) + duration) / 
            this.metrics.totalRequests;
        
        // Update website health
        const domain = new URL(url).hostname;
        if (!this.metrics.websiteHealth[domain]) {
            this.metrics.websiteHealth[domain] = {
                requests: 0,
                successes: 0,
                failures: 0,
                lastSuccess: null,
                lastFailure: null
            };
        }
        
        const siteMetrics = this.metrics.websiteHealth[domain];
        siteMetrics.requests++;
        
        if (success) {
            siteMetrics.successes++;
            siteMetrics.lastSuccess = new Date().toISOString();
        } else {
            siteMetrics.failures++;
            siteMetrics.lastFailure = new Date().toISOString();
        }
    }
    
    getHealthReport() {
        const successRate = (this.metrics.successfulRequests / this.metrics.totalRequests) * 100;
        
        return {
            overall: {
                successRate: successRate.toFixed(2) + '%',
                totalRequests: this.metrics.totalRequests,
                averageResponseTime: this.metrics.averageResponseTime.toFixed(2) + 'ms'
            },
            errors: this.metrics.errorsByType,
            websites: Object.entries(this.metrics.websiteHealth).map(([domain, stats]) => ({
                domain,
                successRate: ((stats.successes / stats.requests) * 100).toFixed(2) + '%',
                ...stats
            }))
        };
    }
    
    exportMetrics() {
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
        const metricsPath = path.join(__dirname, 'metrics', `metrics_${timestamp}.json`);
        
        const metricsDir = path.dirname(metricsPath);
        if (!fs.existsSync(metricsDir)) {
            fs.mkdirSync(metricsDir, { recursive: true });
        }
        
        fs.writeFileSync(metricsPath, JSON.stringify({
            exportedAt: new Date().toISOString(),
            ...this.metrics
        }, null, 2));
        
        logger.info('Metrics exported', { path: metricsPath });
    }
}

const metricsCollector = new MetricsCollector();
```

### 6. **Testing Infrastructure**

#### Unit Tests for Core Functions
```javascript
// tests/scraping.test.js
const { scrapeTopNews, isRetryableError, getErrorType } = require('../snapshot.js');
const puppeteer = require('puppeteer');

describe('Scraping Functions', () => {
    let browser;
    
    beforeAll(async () => {
        browser = await puppeteer.launch({ headless: true });
    });
    
    afterAll(async () => {
        await browser.close();
    });
    
    describe('Error Classification', () => {
        test('should identify retryable errors', () => {
            const retryableError = new Error('socket hang up');
            expect(isRetryableError(retryableError)).toBe(true);
            
            const nonRetryableError = new Error('Invalid selector');
            expect(isRetryableError(nonRetryableError)).toBe(false);
        });
        
        test('should classify error types correctly', () => {
            expect(getErrorType(new Error('socket hang up'))).toBe('NETWORK_DISCONNECTION');
            expect(getErrorType(new Error('timeout'))).toBe('TIMEOUT');
            expect(getErrorType(new Error('ECONNRESET'))).toBe('CONNECTION_RESET');
        });
    });
    
    describe('Scraping Integration', () => {
        test('should handle valid news website', async () => {
            // Mock a simple HTML page
            const mockHtml = `
                <html>
                    <body>
                        <article>
                            <h2>Test News Title</h2>
                            <p>Test summary content</p>
                            <a href="/article/1">Read more</a>
                        </article>
                    </body>
                </html>
            `;
            
            // Test with local server or mock
            // Implementation depends on testing strategy
        }, 30000);
    });
});
```

### 7. **Documentation & Code Standards**

#### JSDoc Documentation
```javascript
/**
 * Scrapes news articles from a given URL with enhanced error handling
 * @param {string} url - The URL to scrape news from
 * @param {Object} options - Configuration options
 * @param {number} [options.maxNews=10] - Maximum number of articles to extract
 * @param {number} [options.timeout=30000] - Request timeout in milliseconds
 * @param {boolean} [options.saveToFile=true] - Whether to save results to file
 * @param {Object} [options.viewport] - Browser viewport configuration
 * @param {number} [options.viewport.width=1920] - Viewport width
 * @param {number} [options.viewport.height=1080] - Viewport height
 * @returns {Promise<Array<Article>>} Array of scraped news articles
 * @throws {ScrapingError} When scraping fails after all retries
 * 
 * @example
 * // Basic usage
 * const articles = await scrapeTopNews('https://example.com');
 * 
 * @example
 * // With custom options
 * const articles = await scrapeTopNews('https://example.com', {
 *   maxNews: 5,
 *   timeout: 15000,
 *   viewport: { width: 1280, height: 720 }
 * });
 */
async function scrapeTopNews(url, options = {}) {
    // Implementation...
}

/**
 * @typedef {Object} Article
 * @property {string} title - Article headline
 * @property {string} link - URL to full article
 * @property {string} summary - Article summary or excerpt
 * @property {string} image - URL to article image
 * @property {string} scrapedAt - ISO timestamp of when article was scraped
 * @property {string} [method] - Scraping method used (puppeteer|http-regex|meta-tags)
 */
```

## üöÄ Implementation Priority

### Phase 1: Critical Fixes (Week 1)
1. ‚úÖ **Enhanced Error Handling**: Implement retry logic with exponential backoff
2. ‚úÖ **Resource Management**: Add proper browser cleanup and connection pooling
3. ‚úÖ **Logging**: Implement structured logging for better debugging

### Phase 2: Reliability Improvements (Week 2)
1. ‚úÖ **Configuration Management**: Environment-based configuration
2. ‚úÖ **Health Monitoring**: Metrics collection and health checks
3. ‚úÖ **Alternative Methods**: Improve HTTP-based fallback scraping

### Phase 3: Quality Assurance (Week 3)
1. ‚úÖ **Testing**: Unit and integration tests
2. ‚úÖ **Documentation**: Comprehensive JSDoc and README updates
3. ‚úÖ **Performance**: Optimize memory usage and response times

## üìä Expected Improvements

| Metric | Current | Target | Improvement |
|--------|---------|--------|-----------|
| Success Rate | ~70% | >95% | +25% |
| Error Recovery | Manual | Automatic | 100% |
| Debug Time | Hours | Minutes | -90% |
| Resource Usage | High | Optimized | -40% |
| Maintainability | Medium | High | +60% |

## üîß Quick Wins

### Immediate Actions (< 1 hour)
1. **Add retry logic** to existing scraping functions
2. **Implement proper browser cleanup** in finally blocks
3. **Add structured logging** for network errors
4. **Update error messages** to be more descriptive

### Environment Variables to Add
```bash
# Add to .env
LOG_LEVEL=debug
MAX_RETRIES=3
SCRAPING_TIMEOUT=30000
MAX_BROWSERS=2
ENABLE_FILE_LOGGING=true
DELAY_BETWEEN_REQUESTS=2000
```

## üéØ Success Metrics

- **Zero unhandled socket hang up errors**
- **95%+ scraping success rate**
- **Sub-5-minute error diagnosis time**
- **Automated error recovery**
- **Comprehensive test coverage (>80%)**

These enhancements will significantly improve the robustness, maintainability, and debuggability of your news scraping system, particularly for handling network issues like the khaleejtimes.com socket hang up error.